{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functions_data_transformation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions_data_transformation.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions_general import *\n",
    "from configurations import *\n",
    "from functions_plots import *\n",
    "\n",
    "SUITE2P_STRUCTURE = {\n",
    "    \"F\": [\"suite2p\", \"plane0\", \"F.npy\"],\n",
    "    \"Fneu\": [\"suite2p\", \"plane0\", \"Fneu.npy\"],\n",
    "    'spks': [\"suite2p\", \"plane0\", \"spks.npy\"],\n",
    "    \"stat\": [\"suite2p\", \"plane0\", \"stat.npy\"],\n",
    "    \"iscell\": [\"suite2p\", \"plane0\", \"iscell.npy\"],\n",
    "    'deltaF': ['suite2p', 'plane0', 'deltaF.npy'],\n",
    "    'ops':[\"suite2p\", \"plane0\", \"ops.npy\"],\n",
    "    'cascade_predictions': ['suite2p', 'plane0', 'predictions_deltaF.npy']\n",
    "}\n",
    "\n",
    "def get_file_name_list(folder_path, file_ending, supress_printing = False): ## accounts for possible errors if deltaF files have been created before\n",
    "    file_names = []\n",
    "    other_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file_ending==\"F.npy\" and file.endswith(file_ending) and not file.endswith(\"deltaF.npy\"):\n",
    "                    file_names.append(os.path.join(root, file))\n",
    "            elif file_ending==\"deltaF.npy\" and file.endswith(file_ending) and not file.endswith(\"predictions_deltaF.npy\"):\n",
    "                    file_names.append(os.path.join(root, file))\n",
    "            elif file_ending==\"predictions_deltaF.npy\" and file.endswith(file_ending):\n",
    "                 file_names.append(os.path.join(root, file))\n",
    "            elif file_ending==\"samples\":\n",
    "                if file.endswith(\"F.npy\") and not file.endswith(\"deltaF.npy\"):\n",
    "                    file_names.append(os.path.join(root, file)[:-21])\n",
    "            else:\n",
    "                 if file.endswith(file_ending): other_files.append(os.path.join(root, file))\n",
    "    if file_ending==\"F.npy\" or file_ending==\"deltaF.npy\" or file_ending==\"predictions_deltaF.npy\":\n",
    "        if not supress_printing:\n",
    "            print(f\"{len(file_names)} {file_ending} files found:\")\n",
    "            print(file_names)\n",
    "        return file_names\n",
    "    elif file_ending==\"samples\":\n",
    "        check_deltaF(file_names)  #checks if deltaf exists, else calculates it\n",
    "        if not supress_printing:\n",
    "            print(f\"{len(file_names)} folders containing {file_ending} found:\")\n",
    "            print(file_names)\n",
    "        return file_names\n",
    "    else:\n",
    "        print(\"Is the file ending spelled right?\")\n",
    "        return other_files\n",
    "\n",
    "def load_npy_array(npy_path):\n",
    "    return np.load(npy_path, allow_pickle=True) #functionally equivalent to np.load(npy_array) but iterable; w/ Pickle\n",
    "\n",
    "def load_npy_df(npy_path):\n",
    "    return pd.DataFrame(np.load(npy_path, allow_pickle=True)) #load suite2p outputs as pandas dataframe\n",
    "\n",
    "def check_deltaF(folder_name_list):\n",
    "    for folder in folder_name_list:\n",
    "        location = os.path.join(folder, *SUITE2P_STRUCTURE[\"deltaF\"])\n",
    "        if os.path.exists(location):\n",
    "            continue\n",
    "        else:\n",
    "            calculate_deltaF(location.replace(\"deltaF.npy\",\"F.npy\"))\n",
    "            if os.path.exists(location):\n",
    "                continue\n",
    "            else:\n",
    "                print(\"something went wrong, please calculate delta F manually by inserting the following code above: \\n F_files = get_file_name_list(folder_path = main_folder, file_ending = 'F.npy') \\n for file in F_files: calculate_deltaF(file)\")\n",
    "\n",
    "def get_sample_dict(main_folder):\n",
    "    '''returns a dictionary of all wells and the corresponding sample/replicate, the samples are sorted by date, everything sampled on the first date is then sample1, on the second date sample2, etc.'''\n",
    "    well_folders = get_file_name_list(main_folder, \"samples\", supress_printing = True)\n",
    "    date_list= []\n",
    "    sample_dict = {}\n",
    "    for well in well_folders:\n",
    "        date_list.append(os.path.basename(well)[:6]) ## append dates\n",
    "    distinct_dates = [i for i in set(date_list)]\n",
    "    distinct_dates.sort(key=lambda x: int(x))\n",
    " \n",
    "    for i1 in range(len(well_folders)):\n",
    "        for i2, date in enumerate(distinct_dates):\n",
    "            if date in well_folders[i1]: # if date in list\n",
    "                sample_dict[well_folders[i1]]=f\"sample_{i2+1}\"\n",
    "    return sample_dict\n",
    "\n",
    "def create_df(suite2p_dict): ## creates df structure for single sample (e.g. well_x) csv file, input is dict resulting from load_suite2p_paths\n",
    "    \"\"\"this is the principle function in which we will create our .csv file structure; and where we will actually use\n",
    "        our detector functions for spike detection and amplitude extraction\"\"\"\n",
    " \n",
    "    ## spike_amplitudes = find_predicted_peaks(suite2p_dict[\"cascade_predictions\"], return_peaks = False) ## removed\n",
    "    # spikes_per_neuron = find_predicted_peaks(suite2p_dict[\"cascade_predictions\"]) ## removed\n",
    " \n",
    "    estimated_spike_total = np.array(summed_spike_probs_per_cell(suite2p_dict[\"cascade_predictions\"]))\n",
    " \n",
    "    basic_stats = basic_stats_per_cell(suite2p_dict[\"cascade_predictions\"])\n",
    "   \n",
    "    ## all columns of created csv below ##\n",
    " \n",
    "    df = pd.DataFrame({\"IsUsed\": suite2p_dict[\"IsUsed\"],\n",
    "                       \"Skew\": suite2p_dict[\"stat\"][\"skew\"],\n",
    "                       \"EstimatedSpikes\": estimated_spike_total,\n",
    "                       \"SD_ES\":basic_stats[1],\n",
    "                       \"cv_ES\":basic_stats[2],\n",
    "                       \"Total Frames\": len(suite2p_dict[\"F\"].T)-64,\n",
    "                       \"SpikesFreq\": (estimated_spike_total / ((len(suite2p_dict[\"F\"].T)-64)) * frame_rate), ## -64 because first and last entries in cascade are NaN, thus not considered in estimated spikes)\n",
    "                       \"group\": suite2p_dict[\"Group\"],\n",
    "                       \"dataset\":suite2p_dict[\"sample\"],\n",
    "                       \"file_name\": suite2p_dict[\"file_name\"]})\n",
    "                      \n",
    "    df.index.set_names(\"NeuronID\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_suite2p_paths(data_folder, groups, main_folder, use_iscell=False):  ## creates a dictionary for the suite2p paths in the given data folder (e.g.: folder for well_x)\n",
    "    \"\"\"here we define our suite2p dictionary from the SUITE2P_STRUCTURE...see above\"\"\"\n",
    "    suite2p_dict = {\n",
    "        \"F\": load_npy_array(os.path.join(data_folder, *SUITE2P_STRUCTURE[\"F\"])),\n",
    "        \"Fneu\": load_npy_array(os.path.join(data_folder, *SUITE2P_STRUCTURE[\"Fneu\"])),\n",
    "        \"stat\": load_npy_df(os.path.join(data_folder, *SUITE2P_STRUCTURE[\"stat\"]))[0].apply(pd.Series),\n",
    "        \"ops\": load_npy_array(os.path.join(data_folder, *SUITE2P_STRUCTURE['ops'])).item(),\n",
    "        \"cascade_predictions\": load_npy_array(os.path.join(data_folder, *SUITE2P_STRUCTURE[\"cascade_predictions\"])),\n",
    "    }\n",
    " \n",
    "    if use_iscell == False:\n",
    "        suite2p_dict[\"IsUsed\"] = [(suite2p_dict[\"stat\"][\"skew\"] >= 1)] \n",
    "        suite2p_dict[\"IsUsed\"] = pd.DataFrame(suite2p_dict[\"IsUsed\"]).iloc[:,0:].values.T\n",
    "        suite2p_dict[\"IsUsed\"] = np.squeeze(suite2p_dict[\"IsUsed\"])\n",
    "    else:\n",
    "        suite2p_dict[\"IsUsed\"] = load_npy_df(os.path.join(path, *SUITE2P_STRUCTURE[\"iscell\"]))[0].astype(bool)\n",
    " \n",
    "    for group in groups: ## creates the group column based on groups list from configurations file\n",
    "        if (str(group)) in data_folder:\n",
    "            suite2p_dict[\"Group\"] = str(group[len(main_folder)+1:])\n",
    " \n",
    "    sample_dict = get_sample_dict(main_folder) ## creates the sample number dict\n",
    "   \n",
    "    suite2p_dict[\"sample\"] = sample_dict[data_folder]  ## gets the sample number for the corresponding well folder from the sample dict\n",
    " \n",
    "    suite2p_dict[\"file_name\"] = str(os.path.join(data_folder, *SUITE2P_STRUCTURE[\"cascade_predictions\"]))\n",
    " \n",
    "    return suite2p_dict\n",
    "\"\"\"\n",
    "Possible to append this function further for synapse exclusion\n",
    " for example, append the document based on \n",
    "suite2p_dict[\"stat\"] using values for [\"skew\"]/[\"npix\"]/[\"compactness\"]\n",
    "\"\"\"\n",
    "\n",
    "#     ImgShape = getImg(suite2p_dict['ops']).shape\n",
    "#     ImgShape = f\"[{ImgShape[0]},{ImgShape[1]}]\"\n",
    "#     ImgShape = [ImgShape] * len(suite2p_dict['IsUsed'])\n",
    "#     df = pd.DataFrame({\"experiment_date\": suite2p_dict[\"experiment_date\"],\n",
    "#                        \"IsUsed\": suite2p_dict[\"IsUsed\"],\n",
    "#                        \"Skew\": suite2p_dict[\"stat\"][\"skew\"],\n",
    "#                        \"ImgShape\": ImgShape,\n",
    "\n",
    "#     # suite2p_dict[\"experiment_date\"] = int(os.path.split(data_folder)[1][8:14]) #TODO change if needed in future\n",
    "#     # suite2p_dict[\"file_name\"] = os.path.split(data_folder)[1]\n",
    "\n",
    "\n",
    "def create_output_csv(input_path, overwrite=False, check_for_iscell=False): ## creates output csv for all wells and saves them in .csv folder\n",
    "    \"\"\"This will create .csv files for each video loaded from out data fram function below.\n",
    "        The structure will consist of columns that list: \"Amplitudes\": spike_amplitudes})\n",
    "        \n",
    "        col1: ROI #, col2: IsUsed (from iscell.npy); boolean, col3: Skew (from stats.npy); could be replaced with any \n",
    "        stat >> compactness, col3: spike frames (relative to input frames), col4: amplitude of each spike detected measured \n",
    "        from the baseline (the median of each trace)\"\"\"\n",
    "    \n",
    "    well_folders = get_file_name_list(input_path, \"samples\", supress_printing = True)\n",
    "\n",
    "    output_path = input_path+r\"\\csv_files\"\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    \n",
    "    for folder in well_folders:\n",
    "        output_directory = (os.path.relpath(folder, input_path)).replace('\\\\', '-')\n",
    "        translated_path = os.path.join(output_path, f\"{output_directory}.csv\")\n",
    "        if os.path.exists(translated_path) and not overwrite:\n",
    "            print(f\"CSV file {translated_path} already exists!\")\n",
    "            continue\n",
    "\n",
    "        output_df = create_df(load_suite2p_paths(folder, groups, input_path))\n",
    "\n",
    "        output_df.to_csv(translated_path)\n",
    "        print(f\"csv created for {folder}\")\n",
    "\n",
    "        suite2p_dict = load_suite2p_paths(folder, groups, input_path, use_iscell=check_for_iscell)\n",
    "        ops = suite2p_dict['ops']\n",
    "        Img = getImg(ops)\n",
    "        scatters, nid2idx, nid2idx_rejected, pixel2neuron = getStats(suite2p_dict['stat'], Img.shape, output_df)\n",
    "\n",
    "        image_save_path = os.path.join(input_path, f\"{folder}_plot.png\")\n",
    "        dispPlot(Img, scatters, nid2idx, nid2idx_rejected, pixel2neuron, suite2p_dict[\"F\"], suite2p_dict[\"Fneu\"], image_save_path)\n",
    "\n",
    "    print(f\"{len(well_folders)} .csv files were saved under {main_folder+r'/csv_files'}\")\n",
    "\n",
    "## create .pkl and final df ##\n",
    "def get_pkl_file_name_list(folder_path): \n",
    "    pkl_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pkl\"):\n",
    "                pkl_files.append(os.path.join(root, file))\n",
    "    return pkl_files\n",
    "\n",
    "def list_all_files_of_type(input_path, filetype):\n",
    "    return [os.path.join(input_path, path) for path in os.listdir(input_path) if path.endswith(filetype)]\n",
    "\n",
    "def csv_to_pickle(main_folder, output_path, overwrite=True):\n",
    "    '''creates pkl, output -> main_folder+r\"\\pkl_files\"'''\n",
    "    csv_files = list_all_files_of_type(main_folder+r\"/csv_files\", \".csv\")\n",
    "    print((csv_files))\n",
    "\n",
    "    if not os.path.exists(main_folder+r\"/pkl_files\"):\n",
    "        os.mkdir(main_folder+r\"/pkl_files\")\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        pkl_path = os.path.join(output_path, \n",
    "                                        f\"{os.path.basename(file[:-4])}\"\n",
    "                                        f\"Dur{int(EXPERIMENT_DURATION)}s\"\n",
    "                                        f\"Int{int(FRAME_INTERVAL*1000)}ms\"\n",
    "                                        f\"Bin{int(BIN_WIDTH*1000)}ms\"\n",
    "                                            + (\"_filtered\" if FILTER_NEURONS else \"\") +\n",
    "                                        \".pkl\")\n",
    "        if os.path.exists(pkl_path) and not overwrite:\n",
    "            print(f\"Processed file {pkl_path} already exists!\")\n",
    "            continue\n",
    "\n",
    "        df.to_pickle(pkl_path)\n",
    "        print(f\"{pkl_path} created\")\n",
    "    print(f\".pkl files saved under {main_folder+r'/pkl_files'}\")\n",
    "\n",
    "def create_final_df(main_folder):\n",
    "    ''' creates the final datat frame (all the wells in one dataframe) from which further analyses can be done'''\n",
    "    pkl_files = get_pkl_file_name_list(main_folder)\n",
    "    df_list = []\n",
    "    for file in pkl_files:\n",
    "        df = pd.read_pickle(file)\n",
    "        df_list.append(df)\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    if len(get_file_name_list(main_folder, \"samples\")) != len(pkl_files):\n",
    "        raise Exception(\"The amount of .pkl files doesn't match the amount of samples, please delete all .csv and .pkl files and start over\")\n",
    "    return final_df\n",
    "    ##alternative df from cell_stats dict, add previous functions back in then\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suite2p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
