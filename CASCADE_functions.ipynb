{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CASCADE_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CASCADE_functions.py\n",
    "## CASCADE functions ##\n",
    "\n",
    "import os, warnings\n",
    "import sys\n",
    "import numpy as np\n",
    "from configurations import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0,cascade_file_path) # cascade2p packages, imported from the downloaded Github repository\n",
    "from cascade2p import cascade # local folder\n",
    "from cascade2p.utils import plot_dFF_traces, plot_noise_level_distribution, plot_noise_matched_ground_truth, calculate_noise_levels\n",
    "\n",
    "\n",
    "def load_neurons_x_time(file_path):\n",
    "    \"\"\"Custom method to load data as 2d array with shape (neurons, nr_timepoints)\"\"\"\n",
    "\n",
    "    if file_path.endswith('.npy'):\n",
    "      traces = np.load(file_path, allow_pickle=True)\n",
    "      # if saved data was a dictionary packed into a numpy array (MATLAB style): unpack\n",
    "      if traces.shape == ():\n",
    "        traces = traces.item()['dF_traces']\n",
    "\n",
    "    else:\n",
    "      raise Exception('This function only supports .npy files.') ## technically also exists for matlab files but dropped here, can be added back in if necessary (see original CASCADE code)\n",
    "\n",
    "    print('Traces standard deviation:', np.nanmean(np.nanstd(traces,axis=1)))\n",
    "    if np.nanmedian(np.nanstd(traces,axis=1)) > 2:\n",
    "      print('Fluctuations in dF/F are very large, probably dF/F is given in percent. Traces are divided by 100.')\n",
    "      return traces/100\n",
    "    else:\n",
    "        return traces\n",
    "\n",
    "def plots_and_basic_info(deltaF_file): ## maybe make into one function with cascade_this, comment what part does what and which can be commented out if not needed\n",
    "\n",
    "    ROI_number = len(np.load(deltaF_file))\n",
    "\n",
    "    try:\n",
    "\n",
    "      print(deltaF_file)\n",
    "      traces = load_neurons_x_time(rf'{deltaF_file}')\n",
    "      print('Number of neurons in dataset:', traces.shape[0])\n",
    "      print('Number of timepoints in dataset:', traces.shape[1])\n",
    "\n",
    "      ## histogram noise level across neurons\n",
    "      warnings.filterwarnings('ignore')\n",
    "      plt.rcParams['figure.figsize'] = [12, 5]\n",
    "      plt.show()\n",
    "      noise_levels = plot_noise_level_distribution(traces,frame_rate)\n",
    "\n",
    "      ## df/f plots\n",
    "      plt.rcParams['figure.figsize'] = [13, 13]\n",
    "      #np.random.seed(3952)\n",
    "      ## plot size calculation, plot 5% of ROIs, minimum 4 (code doesnt work for size < 3), can be removed or replaced by fixed number\n",
    "      plot_number = 6 ## or if fixed percentage plot_number = int(0.05*ROI_number)\n",
    "      if plot_number <4: plot_number = 4 ## can be removed\n",
    "      neuron_indices = np.random.randint(traces.shape[0], size=plot_number)  ## if removed set number here or add plot_number = n at top\n",
    "      time_axis = plot_dFF_traces(traces,neuron_indices,frame_rate)\n",
    "      plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "      print('\\nSomething went wrong!\\nEither the target deltaF_file is missing, in this case please provide the correct location.\\nOr your deltaF_file is not yet completely uploaded, in this case wait until the upload is completed.\\n')\n",
    "      print('Error message: '+str(e))\n",
    "\n",
    "def cascade_this(deltaF_file, nb_neurons):\n",
    "\n",
    "  # try:\n",
    "\n",
    "    print(f\"{deltaF_file}\")\n",
    "    traces = load_neurons_x_time(rf'{deltaF_file}')\n",
    "    noise_levels = calculate_noise_levels(traces, frame_rate)\n",
    "\n",
    "    #@markdown If this takes too long, make sure that the GPU runtime is activated (*Menu > Runtime > Change Runtime Type*).\n",
    "\n",
    "    total_array_size = traces.itemsize*traces.size*64/1e9\n",
    "\n",
    "    # If the expected array size is too large for the Colab Notebook, split up for processing\n",
    "    if total_array_size < 10:\n",
    "\n",
    "      spike_prob = cascade.predict(model_name, traces, model_folder = cascade_file_path+r\"\\Pretrained_models\", verbosity=1)\n",
    "\n",
    "    # Will only be use for large input arrays (long recordings or many neurons)\n",
    "    else:\n",
    "\n",
    "      print(\"Split analysis into chunks in order to fit into Colab memory.\")\n",
    "\n",
    "      # pre-allocate array for results\n",
    "      spike_prob = np.zeros((traces.shape))\n",
    "      # nb of neurons and nb of chuncks\n",
    "      nb_neurons = traces.shape[0]\n",
    "      nb_chunks = int(np.ceil(total_array_size/10))\n",
    "\n",
    "      chunks = np.array_split(range(nb_neurons), nb_chunks)\n",
    "      # infer spike rates independently for each chunk\n",
    "      for part_array in range(nb_chunks):\n",
    "        spike_prob[chunks[part_array],:] = cascade.predict(model_name, traces[chunks[part_array],:])\n",
    "\n",
    "  ## The dF/F traces are shown in blue, the inferred spike probability is plotted in orange (shifted downwards by 1 for better visibility).\n",
    "    print(f\"\\ncurrent file: {deltaF_file}\")\n",
    "    neuron_indices = np.random.randint(traces.shape[0], size=nb_neurons)\n",
    "    time_axis = plot_dFF_traces(traces,neuron_indices,frame_rate,spike_prob,y_range=(-1.5, 3))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ## Plots randomly drawn excerpts from the ground truth, re-sampled at the same frame rate and noise level as a typical recording of the test dataset.\n",
    "    ## The resampled dF/F signal is shown in blue. The true spike rate convolved with a smoothing kernel is shown in orange (shifted downward by 1 for better visibility).\n",
    "    ## This allows to directly compare **data quality** and **possible artifacts** of training dataset (ground truth) and test dataset (your calcium imaging data).\n",
    "\n",
    "    ## Repeatedly execute this cell to plot new examples.\n",
    "\n",
    "    median_noise = np.round(np.maximum(2,np.median(noise_levels)))\n",
    "    nb_traces = 16\n",
    "    duration = max(time_axis) - 64/frame_rate # seconds\n",
    "    plot_noise_matched_ground_truth(model_name, median_noise, frame_rate, nb_traces, duration, cascade_file_path)\n",
    "    plt.show()\n",
    "\n",
    "    #@markdown By default saves as variable **`spike_prob`** both to a *.mat-file and a *.npy-file. You can uncomment the file format that you do not need or leave it as it is.\n",
    "\n",
    "    folder = os.path.dirname(deltaF_file)\n",
    "    file_name = 'predictions_' + os.path.splitext( os.path.basename(deltaF_file))[0]\n",
    "    save_path = os.path.join(folder, file_name)\n",
    "\n",
    "    # save as numpy file\n",
    "    np.save(save_path, spike_prob)\n",
    "    print(f\"saved under {save_path} \\n\")\n",
    "\n",
    "  # except Exception as e:\n",
    "\n",
    "  #   print('\\nSomething went wrong!\\nEither the target file is missing, in this case please provide the correct location.\\nOr your file is not yet completely uploaded, in this case wait until the upload is completed.\\n')\n",
    "  #   print('Error message: '+str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cascade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
