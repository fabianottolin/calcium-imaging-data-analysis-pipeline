{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functions_general.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions_general.py\n",
    "\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from configurations import *\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "\n",
    "\n",
    "def find_predicted_peaks(cascade_predictions, return_peaks = True):\n",
    "## make sure fits peak plots ##\n",
    "    peaks_list = []\n",
    "    amplitudes_list = []\n",
    "\n",
    "    for cell in cascade_predictions:\n",
    "    \n",
    "        peaks, _ = find_peaks(cell, distance = 5)  ## adjust !!!\n",
    "        amplitudes = cell[peaks]\n",
    "\n",
    "        peaks_list.append(peaks)\n",
    "        amplitudes_list.append(amplitudes)\n",
    "\n",
    "\n",
    "    if return_peaks:\n",
    "        return peaks_list\n",
    "    else:\n",
    "        return amplitudes_list\n",
    "\n",
    "\n",
    "def basic_stats_per_cell(predictions_file):\n",
    "    '''returns means, SDs, cvs for all cells in file, mean/SD/cv based on predicited spikes for this cell'''\n",
    "    means = []\n",
    "    sds = []\n",
    "    cvs = []\n",
    "    for cell in predictions_file:\n",
    "        mean=np.nanmean(cell)\n",
    "        means.append(mean)\n",
    "        sd=np.nanstd(cell)\n",
    "        sds.append(sd)\n",
    "        if mean != 0:\n",
    "            cv_cell = sd/mean\n",
    "        else:\n",
    "            cv_cell = np.nan ## cells that don't fire (--> mean spike probability 0) --> makes cv nan\n",
    "        cvs.append(cv_cell)\n",
    "    return means, sds, cvs\n",
    "\n",
    " \n",
    "def summed_spike_probs_per_cell(prediction_deltaF_file):\n",
    "\n",
    "    summed_spike_probs_cell = []\n",
    "\n",
    "    for cell in prediction_deltaF_file:\n",
    "        summed_spike_probs_cell.append(np.nansum(cell))\n",
    "\n",
    "    return summed_spike_probs_cell\n",
    "\n",
    "\n",
    "def calculate_deltaF(F_file):\n",
    "\n",
    "    savepath = rf\"{F_file}\".replace(\"\\\\F.npy\",\"\") ## make savepath original folder, indicates where deltaF.npy is saved\n",
    "\n",
    "    F = np.load(rf\"{F_file}\", allow_pickle=True)\n",
    "\n",
    "    Fneu = np.load(rf\"{F_file[:-4]}\"+\"neu.npy\", allow_pickle=True)\n",
    "\n",
    "    deltaF= []\n",
    "\n",
    "    for f, fneu in zip(F, Fneu):\n",
    "        corrected_trace = f - (0.7*fneu) ## neuropil correction\n",
    "\n",
    "        amount = int(0.125*len(corrected_trace))\n",
    "        middle = 0.5*len(corrected_trace)\n",
    "        F_sample = (np.concatenate((corrected_trace[0:amount], corrected_trace[int(middle-amount/2):int(middle+amount/2)], corrected_trace[len(corrected_trace)-amount:len(corrected_trace)])))  #dynamically chooses beginning, middle, end 12.5%, changeable\n",
    "        F_baseline = np.median(F_sample)\n",
    "        deltaF.append((corrected_trace-F_baseline)/F_baseline)\n",
    "\n",
    "    deltaF = np.array(deltaF)\n",
    "\n",
    "    np.save(f\"{savepath}/deltaF.npy\", deltaF, allow_pickle=True)\n",
    "\n",
    "    print(f\"delta F calculated for {F_file[len(main_folder)+1:-21]}\")\n",
    "\n",
    "    csv_filename = f\"{F_file[len(main_folder)+1:-21]}\".replace(\"\\\\\", \"-\") ## prevents backslahes being replaced in rest of code\n",
    "\n",
    "    if not os.path.exists(main_folder + r'\\csv_files_deltaF'): ## creates directory if it doesn't exist\n",
    "        os.mkdir(main_folder + r'\\csv_files_deltaF')\n",
    "\n",
    "    np.savetxt(f\"{main_folder}/csv_files_deltaF/{csv_filename}.csv\", deltaF, delimiter=\";\") ### can be commented out if you don't want to save deltaF as .csv files (additionally to .npy)\n",
    "\n",
    "    ## if done by pandas, version needs to be checked, np.savetxt might be enough anyways ##\n",
    "    # df = pd.DataFrame(deltaF)\n",
    "    # df.to_csv(f\"{main_folder}\"+\"/csv_files/\"+f\"{file[len(main_folder)+1:-21]}\"+\".csv\", index = False, header = False)\n",
    "\n",
    "    print(f\"delta F traces saved as deltaF.npy under {savepath}\\n\")\n",
    "\n",
    "from functions_data_transformation import get_file_name_list\n",
    "\n",
    "def overview(main_folder, groups):\n",
    "    dictionary_list = []\n",
    "    for group in groups:\n",
    "        groups_predictions_deltaF_files = get_file_name_list(folder_path = group, file_ending = \"predictions_deltaF.npy\", supress_printing = True)\n",
    "        for file in groups_predictions_deltaF_files:\n",
    "            array = np.load(rf\"{file}\")\n",
    "            neuron_count = len(array)\n",
    "            estimated_spikes = []\n",
    "            for i in range(len(array)):\n",
    "                estimated_spikes.append(np.nansum(array[i]))\n",
    "            total_estimated_spikes = round(sum(estimated_spikes), 2)\n",
    "            dictionary_list.append({'Prediction_File': file[len(main_folder)+1:], 'Neuron_Count': neuron_count, 'Total Estimated Spikes': total_estimated_spikes, \"Group\":group[len(main_folder)+1:]})\n",
    "    df = pd.DataFrame(dictionary_list)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cascade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
